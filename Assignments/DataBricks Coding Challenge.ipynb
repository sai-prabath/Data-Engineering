{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["vEQWjavQI9UF"],"toc_visible":true,"mount_file_id":"1qL5dEqgRJkQus2mHUAHIL0_lrrXJzWuO","authorship_tag":"ABX9TyOMpqFU6AUua5YcK4AuioIJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Setup**"],"metadata":{"id":"9OSbWCPUI29G"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-DH90r4AIePt","executionInfo":{"status":"ok","timestamp":1727083468748,"user_tz":-330,"elapsed":8380,"user":{"displayName":"sai prabath","userId":"06763418138744745121"}},"outputId":"224feec6-44c1-43c5-d5a6-6ab63c2033d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Collecting delta-spark\n","  Downloading delta_spark-3.2.0-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (3.5.2)\n","Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (8.5.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark) (3.20.2)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark<3.6.0,>=3.5.0->delta-spark) (0.10.9.7)\n","Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n","Installing collected packages: delta-spark\n","Successfully installed delta-spark-3.2.0\n"]}],"source":["! pip install pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('coding_challenge').getOrCreate()\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","import pandas as pd\n","\n","!pip install delta-spark\n"]},{"cell_type":"markdown","source":["# **Vehicle Maintenance Data Ingestion**"],"metadata":{"id":"vEQWjavQI9UF"}},{"cell_type":"code","source":["# Task 1\n","\n","import os\n","from delta.tables import DeltaTable\n","\n","csv_file_path = \"/content/drive/MyDrive/DataEngineering/DataBricks_Coding_Challenge/vehicle_maintenance.csv\"\n","\n","if os.path.exists(csv_file_path):\n","  try:\n","    df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n","    df.show()\n","    delta_table_path = \"/content/vehicle_maintenance\"\n","    df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n","    print(\"CSV data ingested into Delta table.\")\n","  except Exception as e:\n","    print(f\"Error reading CSV file: {e}\")\n","else:\n","    print(\"CSV file does not exist.\")\n"],"metadata":{"id":"zEwYjieXJHwd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 2\n","\n","# Filter for valid records (positive values for ServiceCost and Mileage)\n","cleaned_df = df.filter((col(\"ServiceCost\") > 0) & (col(\"Mileage\") > 0))\n","\n","# Remove duplicates based on VehicleID and Date\n","cleaned_df = cleaned_df.dropDuplicates([\"VehicleID\", \"Date\"])\n","\n","# Save cleaned data to a new Delta table\n","cleaned_delta_table_path = '/content/delta/cleaned_vehicle_maintenance'\n","cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(cleaned_delta_table_path)\n","\n","print(\"Data cleaned and saved to new Delta table.\")\n"],"metadata":{"id":"1EsPG5BROg9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 3\n","\n","# Calculate total maintenance cost for each vehicle\n","total_cost_df = cleaned_df.groupBy(\"VehicleID\").sum(\"ServiceCost\").withColumnRenamed(\"sum(ServiceCost)\", \"TotalMaintenanceCost\")\n","\n","# Identify vehicles that have exceeded 30,000 miles\n","high_mileage_df = cleaned_df.filter(col(\"Mileage\") > 30000)\n","\n","# Save analysis results to Delta tables\n","total_cost_table_path = \"/content/delta/vehicle_maintenance_total_cost\"\n","high_mileage_table_path = \"/content/delta/vehicle_high_mileage\"\n","\n","total_cost_df.write.format(\"delta\").mode(\"overwrite\").save(total_cost_table_path)\n","high_mileage_df.write.format(\"delta\").mode(\"overwrite\").save(high_mileage_table_path)\n","\n","print(\"Analysis results saved to Delta tables.\")\n"],"metadata":{"id":"f6if7W_qPFvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 4\n","\n","# Perform VACUUM to clean up old data\n","delta_table = DeltaTable.forPath(spark, cleaned_delta_table_path)\n","delta_table.vacuum(retentionHours=168)  # Retains last 7 days (168 hours) by default\n","\n","# View Delta table history\n","spark.sql(f\"DESCRIBE HISTORY delta.`{cleaned_delta_table_path}`\").show()\n","\n","print(\"Data governance tasks completed.\")\n"],"metadata":{"id":"haZ2ulhgPS_z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Movie Ratings Data Ingestion**"],"metadata":{"id":"-_JL7M3gJAvO"}},{"cell_type":"code","source":["# Task 1\n","\n","spark = SparkSession.builder \\\n","    .appName(\"MovieRatings\") \\\n","    .getOrCreate()\n","\n","# Upload CSV to DBFS\n","dbutils.fs.cp(\"file:/content/movie_ratings.csv\", \"dbfs:/Filestore/movie_ratings.csv\")\n","\n","# Read the CSV file into Spark DataFrame\n","try:\n","    movie_ratings_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/Filestore/movie_ratings.csv\")\n","    movie_ratings_df.show()\n","\n","    # Check for missing or inconsistent data (ratings outside of 1-5)\n","    invalid_data_df = movie_ratings_df.filter((col(\"Rating\").cast(IntegerType()) < 1) | (col(\"Rating\").cast(IntegerType()) > 5) | col(\"Rating\").isNull())\n","\n","    if invalid_data_df.count() > 0:\n","        print(\"Invalid or missing data found:\")\n","        invalid_data_df.show()\n","    else:\n","        # Write valid data to Delta table\n","        delta_table_path = \"dbfs:/Filestore/delta_movie_ratings\"\n","        movie_ratings_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n","        print(\"Movie ratings ingested into Delta table.\")\n","except Exception as e:\n","    print(f\"Error during ingestion: {e}\")"],"metadata":{"id":"spZFLHAkPjQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 2\n","\n","# Filter ratings between 1 and 5\n","cleaned_df = movie_ratings_df.filter((col(\"Rating\").cast(IntegerType()) >= 1) & (col(\"Rating\").cast(IntegerType()) <= 5))\n","\n","# Remove duplicates based on UserID and MovieID\n","cleaned_df = cleaned_df.dropDuplicates([\"UserID\", \"MovieID\"])\n","\n","# Write cleaned data to a new Delta table\n","cleaned_delta_table_path = \"dbfs:/Filestore/cleaned_delta_movie_ratings\"\n","cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(cleaned_delta_table_path)\n","print(\"Cleaned data saved to Delta table.\")"],"metadata":{"id":"w_n60d6nQVA8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 3\n","\n","from pyspark.sql.functions import avg\n","\n","# Calculate the average rating for each movie\n","avg_ratings_df = cleaned_df.groupBy(\"MovieID\").agg(avg(\"Rating\").alias(\"AvgRating\"))\n","\n","# Identify movies with highest and lowest average ratings\n","highest_rated_movie = avg_ratings_df.orderBy(col(\"AvgRating\").desc()).limit(1)\n","lowest_rated_movie = avg_ratings_df.orderBy(col(\"AvgRating\").asc()).limit(1)\n","\n","# Display results\n","print(\"Highest Rated Movie:\")\n","highest_rated_movie.show()\n","\n","print(\"Lowest Rated Movie:\")\n","lowest_rated_movie.show()\n","\n","# Save analysis results to a Delta table\n","analysis_delta_table_path = \"dbfs:/Filestore/movie_rating_analysis\"\n","avg_ratings_df.write.format(\"delta\").mode(\"overwrite\").save(analysis_delta_table_path)\n","print(\"Movie rating analysis saved to Delta table.\")\n"],"metadata":{"id":"6P86c6HdQggU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 4\n","\n","# Step 1: Update a few ratings\n","cleaned_df.filter(col(\"MovieID\") == \"M001\").withColumn(\"Rating\", col(\"Rating\") + 1).write.format(\"delta\").mode(\"overwrite\").save(cleaned_delta_table_path)\n","\n","# Step 2: Time travel to previous version (version 0)\n","original_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(cleaned_delta_table_path)\n","print(\"Original Ratings:\")\n","original_df.show()\n","\n","# Step 3: View the history of changes\n","spark.sql(f\"DESCRIBE HISTORY delta.`{cleaned_delta_table_path}`\").show()\n"],"metadata":{"id":"57rpsJhwQpr0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 5\n","\n","# Z-Ordering on MovieID\n","spark.sql(f\"OPTIMIZE delta.`{cleaned_delta_table_path}` ZORDER BY MovieID\")\n","\n","# Compact data using OPTIMIZE\n","spark.sql(f\"OPTIMIZE delta.`{cleaned_delta_table_path}`\")\n","\n","# Clean up old versions with VACUUM\n","spark.sql(f\"VACUUM delta.`{cleaned_delta_table_path}` RETAIN 0 HOURS\")"],"metadata":{"id":"kz7Nd6erQyaz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Data Ingestion - Reading Data from Various Formats**"],"metadata":{"id":"MBVqyWZVJBit"}},{"cell_type":"code","source":["# Task 1\n","\n","# Reading CSV data for student information\n","csv_path = \"dbfs:/Filestore/student_info.csv\"\n","student_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_path)\n","student_df.show()\n","\n","# Reading JSON data for city information\n","json_path = \"dbfs:/Filestore/city_info.json\"\n","city_df = spark.read.json(json_path)\n","city_df.show()\n","\n","# Reading Parquet data for hospitals\n","parquet_path = \"dbfs:/Filestore/hospital_info.parquet\"\n","hospital_df = spark.read.parquet(parquet_path)\n","hospital_df.show()\n","\n","# Reading Delta table with error handling\n","delta_table_path = \"dbfs:/Filestore/delta_hospital\"\n","\n","try:\n","    delta_df = spark.read.format(\"delta\").load(delta_table_path)\n","    delta_df.show()\n","except Exception as e:\n","    print(f\"Error reading Delta table: {e}\")\n","\n"],"metadata":{"id":"oZ2OpadGQ_Fr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Task 2\n","\n","# Writing student data to CSV\n","student_df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/Filestore/student_output.csv\")\n","\n","# Writing city data to JSON\n","city_df.write.format(\"json\").mode(\"overwrite\").save(\"dbfs:/Filestore/city_output.json\")\n","\n","# Writing hospital data to Parquet\n","hospital_df.write.format(\"parquet\").mode(\"overwrite\").save(\"dbfs:/Filestore/hospital_output.parquet\")\n","\n","# Writing hospital data to Delta table\n","hospital_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/Filestore/delta_hospital_output\")\n"],"metadata":{"id":"Ne6prZZMRfSL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Notebook A**"],"metadata":{"id":"f2QId_RuRw5E"}},{"cell_type":"code","source":["# Task 3\n","\n","# Reading, cleaning, and saving student data as Delta\n","csv_path = \"dbfs:/Filestore/student_info.csv\"\n","student_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_path)\n","\n","# Clean data: Remove duplicates\n","cleaned_student_df = student_df.dropDuplicates()\n","\n","# Save cleaned data as Delta\n","cleaned_student_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/Filestore/delta_cleaned_student\")\n"],"metadata":{"id":"FaAfluF9RnC8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Notebook B**"],"metadata":{"id":"jsplbEHoR3Qc"}},{"cell_type":"code","source":["# Reading Delta table and performing analysis\n","delta_student_df = spark.read.format(\"delta\").load(\"dbfs:/Filestore/delta_cleaned_student\")\n","\n","# Calculate average score\n","avg_score_df = delta_student_df.groupBy(\"Class\").avg(\"Score\")\n","\n","# Save analysis results to Delta table\n","avg_score_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/Filestore/delta_avg_score\")\n"],"metadata":{"id":"sHrEEUSmR99U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Running Notebook B from Notebook A**"],"metadata":{"id":"hAvvk7J1R5Tr"}},{"cell_type":"code","source":["# Notebook A - running Notebook B\n","dbutils.notebook.run(\"NotebookB\", 60)  # Timeout of 60 seconds\n"],"metadata":{"id":"j-L0Z02_SBSM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Additional Tasks**"],"metadata":{"id":"wUb5Kgh3JCMl"}},{"cell_type":"code","source":["# Optimization, Z-Ordering, and Vacuum Tasks\n","\n","# Optimize the Delta table\n","spark.sql(\"OPTIMIZE delta.`dbfs:/Filestore/delta_hospital_output`\")\n","\n","# Z-ordering on CityName column\n","spark.sql(\"OPTIMIZE delta.`dbfs:/Filestore/delta_hospital_output` ZORDER BY (CityName)\")\n","\n","# Vacuum Delta table to remove old files\n","spark.sql(\"VACUUM delta.`dbfs:/Filestore/delta_hospital_output` RETAIN 7 HOURS\")\n"],"metadata":{"id":"E1oQx2owS5M7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Exercise 1: Creating a Complete ETL Pipeline using Delta Live Tables (DLT)\n","\n","import dlt\n","from pyspark.sql.functions import col\n","\n","# Raw Transactions Table\n","@dlt.table(name=\"raw_transactions\")\n","def raw_transactions():\n","    return (spark.read.format(\"csv\")\n","            .option(\"header\", \"true\")\n","            .load(\"dbfs:/Filestore/transactions.csv\"))\n","\n","# Transformed Transactions Table\n","@dlt.table(name=\"transformed_transactions\")\n","def transformed_transactions():\n","    return (dlt.read(\"raw_transactions\")\n","            .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")))\n","\n","# Create Raw Transactions Table\n","spark.sql(\"\"\"\n","CREATE OR REPLACE TABLE raw_transactions\n","AS SELECT * FROM csv.`dbfs:/Filestore/transactions.csv`;\n","\"\"\")\n","\n","# Create Transformed Transactions Table\n","spark.sql(\"\"\"\n","#CREATE OR REPLACE TABLE transformed_transactions\n","#AS SELECT *, Quantity * Price AS TotalAmount\n","#FROM raw_transactions;\n","\"\"\")\n"],"metadata":{"id":"spUQjKYVTBrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Exercise 2: Delta Lake Operations (Read, Write, Update, Delete, Merge)\n","\n","# Read Delta table in PySpark\n","df = spark.read.format(\"delta\").load(\"dbfs:/Filestore/delta_transactions\")\n","df.show(5)\n","\n","# SQL method\n","spark.sql(\"SELECT * FROM delta.`dbfs:/Filestore/delta_transactions` LIMIT 5\").show()\n","\n","# Append new transactions in PySpark\n","new_data = [(6, \"2024-09-06\", \"C005\", \"Keyboard\", 4, 100),\n","            (7, \"2024-09-07\", \"C006\", \"Mouse\", 10, 20)]\n","new_df = spark.createDataFrame(new_data, schema=df.schema)\n","\n","new_df.write.format(\"delta\").mode(\"append\").save(\"dbfs:/Filestore/delta_transactions\")\n","\n","# PySpark Update\n","spark.sql(\"UPDATE delta.`dbfs:/Filestore/delta_transactions` SET Price = 1300 WHERE Product = 'Laptop'\")\n","\n","# Merging data into Delta table\n","merge_data = [(1, \"2024-09-01\", \"C001\", \"Laptop\", 1, 1250),  # Updated\n","              (8, \"2024-09-08\", \"C007\", \"Charger\", 2, 30)]   # New\n","\n","merge_df = spark.createDataFrame(merge_data, schema=df.schema)\n","\n","merge_df.createOrReplaceTempView(\"updates\")\n","\n","spark.sql(\"\"\"\n","MERGE INTO delta.`dbfs:/Filestore/delta_transactions` AS t\n","USING updates AS u\n","ON t.TransactionID = u.TransactionID\n","WHEN MATCHED THEN UPDATE SET *\n","WHEN NOT MATCHED THEN INSERT *\n","\"\"\")\n"],"metadata":{"id":"xXFEu_D3TbZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Exercise 3: Delta Lake - History, Time Travel, and Vacuum\n","\n","# View Delta table history\n","spark.sql(\"DESCRIBE HISTORY delta.`dbfs:/Filestore/delta_transactions`\").show()\n","\n","# Time travel using version\n","spark.read.format(\"delta\").option(\"versionAsOf\", 5).load(\"dbfs:/Filestore/delta_transactions\").show()\n","\n","# Time travel using timestamp\n","spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-09-07 12:00:00\").load(\"dbfs:/Filestore/delta_transactions\").show()\n","\n","# Vacuum with retention of 7 days\n","spark.sql(\"VACUUM delta.`dbfs:/Filestore/delta_transactions` RETAIN 7 HOURS\")\n","\n","# Convert Parquet to Delta\n","parquet_path = \"dbfs:/Filestore/transactions_parquet\"\n","delta_path = \"dbfs:/Filestore/delta_transactions_from_parquet\"\n","\n","spark.read.format(\"parquet\").load(parquet_path).write.format(\"delta\").save(delta_path)\n"],"metadata":{"id":"mmK8wCpQUlwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Exercise 4: Incremental Load Pattern using Delta Lake\n","\n","# Filter data for the first three days\n","initial_data = df.filter(df.TransactionDate <= \"2024-09-03\")\n","initial_data.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/Filestore/delta_initial_transactions\")\n","\n","# Load new data (next four days)\n","new_data = df.filter(df.TransactionDate > \"2024-09-03\")\n","new_data.write.format(\"delta\").mode(\"append\").save(\"dbfs:/Filestore/delta_incremental_transactions\")\n","\n","# Incremental load: Read only new data (after 2024-09-03)\n","incremental_df = spark.read.format(\"delta\").load(\"dbfs:/Filestore/delta_incremental_transactions\")\n","incremental_df.show()\n","\n","# Check Delta table history\n","spark.sql(\"DESCRIBE HISTORY delta.`dbfs:/Filestore/delta_incremental_transactions`\").show()\n"],"metadata":{"id":"hdiCEdv9UvHb"},"execution_count":null,"outputs":[]}]}